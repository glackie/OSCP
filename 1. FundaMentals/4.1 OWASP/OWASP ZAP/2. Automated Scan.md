

Lets perform an automated scan. Click the big Automated Scan button and input your target.

![](https://lh4.googleusercontent.com/1NsFAThXFHd7PdAR9C1puIflm8QYQJrxZ0hEC1me4aZKxlfuagRPJHOYdo0RLzmLBMXfWmgUvSOwBLHZkFR_rAANQEb3AG9HkdYyEBsjCr9rDPxGPtQ73SPszwO2BhtrWOR6l2Wa)

The automated scan performs both passive and automated scans to build a sitemap and detect vulnerabilities.

On the next page you may see the options to select either to use “traditional spider” or “Ajax spider”.  

A traditional spider scan is a passive scan that enumerates links and directories of the website. It builds a website index without brute-forcing. This is much quieter than a brute-force attack and can still net a login page or other juicy details, but is not as comprehensive as a bruteforce.  

The Ajax Spider is an add-on that integrates in ZAP a crawler of AJAX rich sites called Crawljax. You can use it in conjunction with the traditional spider for better results. It uses your web browser and proxy.  

The easiest way to use the Ajax Spider is with HTMLUnit.   

To install HTML Unit use the command

sudo apt install libjenkins-htmlunit-core-js-java

And then select HtmlUnity from the Ajax Spider Dropdown. 

Both utilities can further be configured in the options menu (Ctrl+Alt+O)  

Example Automated Scan Output:![](https://lh5.googleusercontent.com/rmYUE3qmjalywZ1gDsYhXtmOlJ5T8ei4OXXV2_0KhK7VPODDLtAoT-c9uBG-CC24Ivf1JkEYsF2irOJClEBqBPxzsdOX2r_ZUnf4i2ed5UZ5Qpl872IENhxcWRVhivehiJb4mV-B)

With very minimal setup we were able to do an automated scan that gave us a sitemap and a handful of vulnerabilities.